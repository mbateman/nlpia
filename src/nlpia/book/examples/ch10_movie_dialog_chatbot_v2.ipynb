{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nlpia.loaders import get_data\n",
    "\n",
    "df = get_data('moviedialog')\n",
    "df.columns = 'statement reply'.split()\n",
    "df = df.dropna()\n",
    "input_texts, target_texts = [], []  # <1>\n",
    "start_token, stop_token = '\\t\\n'  # <3>\n",
    "input_vocab = set()  # <2>\n",
    "output_vocab = set(start_token + stop_token)\n",
    "n_samples = min(100000, len(df))  # <4>\n",
    "\n",
    "df['target'] = start_token + df.reply + stop_token \n",
    "[input_vocab.update(set(statement)) for statement in df.statement]\n",
    "[output_vocab.update(set(reply)) for reply in df.reply]\n",
    "input_vocab = tuple(sorted(input_vocab)) #<6>\n",
    "output_vocab = tuple(sorted(output_vocab))\n",
    "\n",
    "max_encoder_seq_len = df.statement.str.len().max()\n",
    "# max_encoder_seq_len\n",
    "# 100\n",
    "max_decoder_seq_len = df.target.str.len().max()\n",
    "# max_decoder_seq_len\n",
    "# 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # <1> # noqa\n",
    "\n",
    "encoder_input_onehot = np.zeros(\n",
    "    (len(df), max_encoder_seq_len, len(input_vocab)),\n",
    "    dtype='float32')  # <2>\n",
    "decoder_input_onehot = np.zeros(\n",
    "    (len(df), max_decoder_seq_len, len(output_vocab)),\n",
    "    dtype='float32')\n",
    "decoder_target_onehot = np.zeros(\n",
    "    (len(df), max_decoder_seq_len, len(output_vocab)),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(\n",
    "        zip(df.statement, df.target)):  # <3>\n",
    "    for t, c in enumerate(input_text):  # <4>\n",
    "        k = input_vocab.index(c)\n",
    "        encoder_input_onehot[i, t, k] = 1.  # <5>\n",
    "    k = np.array([output_vocab.index(c) for c in target_text])\n",
    "    decoder_input_onehot[i, np.arange(len(target_text)), k] = 1.\n",
    "    decoder_target_onehot[i, np.arange(len(target_text) - 1), k[1:]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/miniconda3/envs/nlpiaenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-04-14 10:24:04,620 WARNING:     tensorflow:323:            new_func From /usr/local/miniconda3/envs/nlpiaenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/miniconda3/envs/nlpiaenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-04-14 10:24:05,096 WARNING:     tensorflow:323:            new_func From /usr/local/miniconda3/envs/nlpiaenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/miniconda3/envs/nlpiaenv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "2019-04-14 10:24:05,187 WARNING:     tensorflow:323:            new_func From /usr/local/miniconda3/envs/nlpiaenv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 57915 samples, validate on 6435 samples\n",
      "Epoch 1/100\n",
      "57915/57915 [==============================] - 147s 3ms/step - loss: 0.7526 - acc: 0.1223 - val_loss: 0.6378 - val_acc: 0.1565\n",
      "Epoch 2/100\n",
      "57915/57915 [==============================] - 147s 3ms/step - loss: 0.5852 - acc: 0.1635 - val_loss: 0.5670 - val_acc: 0.1761\n",
      "Epoch 3/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.5354 - acc: 0.1769 - val_loss: 0.5353 - val_acc: 0.1845\n",
      "Epoch 4/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.5088 - acc: 0.1840 - val_loss: 0.5177 - val_acc: 0.1892\n",
      "Epoch 5/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.4922 - acc: 0.1884 - val_loss: 0.5049 - val_acc: 0.1926\n",
      "Epoch 6/100\n",
      "57915/57915 [==============================] - 141s 2ms/step - loss: 0.4799 - acc: 0.1916 - val_loss: 0.4954 - val_acc: 0.1956\n",
      "Epoch 7/100\n",
      "57915/57915 [==============================] - 141s 2ms/step - loss: 0.4707 - acc: 0.1942 - val_loss: 0.4892 - val_acc: 0.1972\n",
      "Epoch 8/100\n",
      "57915/57915 [==============================] - 141s 2ms/step - loss: 0.4633 - acc: 0.1962 - val_loss: 0.4844 - val_acc: 0.1987\n",
      "Epoch 9/100\n",
      "57915/57915 [==============================] - 141s 2ms/step - loss: 0.4572 - acc: 0.1979 - val_loss: 0.4803 - val_acc: 0.2000\n",
      "Epoch 10/100\n",
      "57915/57915 [==============================] - 141s 2ms/step - loss: 0.4520 - acc: 0.1993 - val_loss: 0.4791 - val_acc: 0.1999\n",
      "Epoch 11/100\n",
      "57915/57915 [==============================] - 141s 2ms/step - loss: 0.4475 - acc: 0.2006 - val_loss: 0.4755 - val_acc: 0.2012\n",
      "Epoch 12/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4443 - acc: 0.2015 - val_loss: 0.4835 - val_acc: 0.1989\n",
      "Epoch 13/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4412 - acc: 0.2023 - val_loss: 0.4736 - val_acc: 0.2016\n",
      "Epoch 14/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4370 - acc: 0.2034 - val_loss: 0.4711 - val_acc: 0.2023\n",
      "Epoch 15/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4339 - acc: 0.2044 - val_loss: 0.4710 - val_acc: 0.2028\n",
      "Epoch 16/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.4313 - acc: 0.2051 - val_loss: 0.4704 - val_acc: 0.2029\n",
      "Epoch 17/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4288 - acc: 0.2060 - val_loss: 0.4695 - val_acc: 0.2031\n",
      "Epoch 18/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4265 - acc: 0.2065 - val_loss: 0.4687 - val_acc: 0.2032\n",
      "Epoch 19/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4243 - acc: 0.2077 - val_loss: 0.4685 - val_acc: 0.2039\n",
      "Epoch 20/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4224 - acc: 0.2078 - val_loss: 0.4685 - val_acc: 0.2037\n",
      "Epoch 21/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.4205 - acc: 0.2082 - val_loss: 0.4682 - val_acc: 0.2036\n",
      "Epoch 22/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.4187 - acc: 0.2086 - val_loss: 0.4685 - val_acc: 0.2036\n",
      "Epoch 23/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.4170 - acc: 0.2091 - val_loss: 0.4685 - val_acc: 0.2034\n",
      "Epoch 24/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.4154 - acc: 0.2097 - val_loss: 0.4691 - val_acc: 0.2038\n",
      "Epoch 25/100\n",
      "57915/57915 [==============================] - 146s 3ms/step - loss: 0.4140 - acc: 0.2100 - val_loss: 0.4692 - val_acc: 0.2037\n",
      "Epoch 26/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.4126 - acc: 0.2105 - val_loss: 0.4695 - val_acc: 0.2034\n",
      "Epoch 27/100\n",
      "57915/57915 [==============================] - 144s 2ms/step - loss: 0.4112 - acc: 0.2108 - val_loss: 0.4692 - val_acc: 0.2040\n",
      "Epoch 28/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4099 - acc: 0.2113 - val_loss: 0.4699 - val_acc: 0.2038\n",
      "Epoch 29/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.4086 - acc: 0.2116 - val_loss: 0.4702 - val_acc: 0.2035\n",
      "Epoch 30/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.4075 - acc: 0.2119 - val_loss: 0.4706 - val_acc: 0.2039\n",
      "Epoch 31/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.4062 - acc: 0.2124 - val_loss: 0.4709 - val_acc: 0.2036\n",
      "Epoch 32/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.4052 - acc: 0.2126 - val_loss: 0.4714 - val_acc: 0.2036\n",
      "Epoch 33/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.4041 - acc: 0.2129 - val_loss: 0.4721 - val_acc: 0.2038\n",
      "Epoch 34/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.4032 - acc: 0.2133 - val_loss: 0.4729 - val_acc: 0.2034\n",
      "Epoch 35/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.4022 - acc: 0.2135 - val_loss: 0.4728 - val_acc: 0.2037\n",
      "Epoch 36/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.4013 - acc: 0.2137 - val_loss: 0.4744 - val_acc: 0.2033\n",
      "Epoch 37/100\n",
      "57915/57915 [==============================] - 141s 2ms/step - loss: 0.4003 - acc: 0.2141 - val_loss: 0.4740 - val_acc: 0.2034\n",
      "Epoch 38/100\n",
      "57915/57915 [==============================] - 144s 2ms/step - loss: 0.3994 - acc: 0.2144 - val_loss: 0.4747 - val_acc: 0.2033\n",
      "Epoch 39/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3986 - acc: 0.2145 - val_loss: 0.4753 - val_acc: 0.2032\n",
      "Epoch 40/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3978 - acc: 0.2147 - val_loss: 0.4760 - val_acc: 0.2030\n",
      "Epoch 41/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3970 - acc: 0.2151 - val_loss: 0.4767 - val_acc: 0.2031\n",
      "Epoch 42/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3963 - acc: 0.2152 - val_loss: 0.4765 - val_acc: 0.2029\n",
      "Epoch 43/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3955 - acc: 0.2153 - val_loss: 0.4783 - val_acc: 0.2030\n",
      "Epoch 44/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3948 - acc: 0.2156 - val_loss: 0.4784 - val_acc: 0.2028\n",
      "Epoch 45/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3941 - acc: 0.2158 - val_loss: 0.4790 - val_acc: 0.2025\n",
      "Epoch 46/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3934 - acc: 0.2161 - val_loss: 0.4790 - val_acc: 0.2024\n",
      "Epoch 47/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3927 - acc: 0.2163 - val_loss: 0.4804 - val_acc: 0.2024\n",
      "Epoch 48/100\n",
      "57915/57915 [==============================] - 141s 2ms/step - loss: 0.3922 - acc: 0.2164 - val_loss: 0.4813 - val_acc: 0.2021\n",
      "Epoch 49/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3915 - acc: 0.2166 - val_loss: 0.4807 - val_acc: 0.2023\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3909 - acc: 0.2168 - val_loss: 0.4812 - val_acc: 0.2020\n",
      "Epoch 51/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3904 - acc: 0.2169 - val_loss: 0.4823 - val_acc: 0.2023\n",
      "Epoch 52/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3898 - acc: 0.2171 - val_loss: 0.4827 - val_acc: 0.2020\n",
      "Epoch 53/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3892 - acc: 0.2173 - val_loss: 0.4831 - val_acc: 0.2020\n",
      "Epoch 54/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3887 - acc: 0.2175 - val_loss: 0.4836 - val_acc: 0.2020\n",
      "Epoch 55/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3882 - acc: 0.2176 - val_loss: 0.4850 - val_acc: 0.2015\n",
      "Epoch 56/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3876 - acc: 0.2178 - val_loss: 0.4838 - val_acc: 0.2020\n",
      "Epoch 57/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3871 - acc: 0.2179 - val_loss: 0.4848 - val_acc: 0.2017\n",
      "Epoch 58/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3867 - acc: 0.2181 - val_loss: 0.4851 - val_acc: 0.2015\n",
      "Epoch 59/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3861 - acc: 0.2183 - val_loss: 0.4858 - val_acc: 0.2018\n",
      "Epoch 60/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3857 - acc: 0.2184 - val_loss: 0.4859 - val_acc: 0.2015\n",
      "Epoch 61/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3852 - acc: 0.2186 - val_loss: 0.4860 - val_acc: 0.2018\n",
      "Epoch 62/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3848 - acc: 0.2186 - val_loss: 0.4869 - val_acc: 0.2014\n",
      "Epoch 63/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3842 - acc: 0.2187 - val_loss: 0.4880 - val_acc: 0.2013\n",
      "Epoch 64/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3839 - acc: 0.2190 - val_loss: 0.4886 - val_acc: 0.2013\n",
      "Epoch 65/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3833 - acc: 0.2191 - val_loss: 0.4880 - val_acc: 0.2012\n",
      "Epoch 66/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3829 - acc: 0.2192 - val_loss: 0.4894 - val_acc: 0.2006\n",
      "Epoch 67/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3825 - acc: 0.2193 - val_loss: 0.4903 - val_acc: 0.2008\n",
      "Epoch 68/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3821 - acc: 0.2195 - val_loss: 0.4891 - val_acc: 0.2011\n",
      "Epoch 69/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3816 - acc: 0.2196 - val_loss: 0.4902 - val_acc: 0.2008\n",
      "Epoch 70/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3813 - acc: 0.2197 - val_loss: 0.4905 - val_acc: 0.2007\n",
      "Epoch 71/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3809 - acc: 0.2198 - val_loss: 0.4904 - val_acc: 0.2010\n",
      "Epoch 72/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3806 - acc: 0.2199 - val_loss: 0.4909 - val_acc: 0.2007\n",
      "Epoch 73/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3802 - acc: 0.2201 - val_loss: 0.4918 - val_acc: 0.2007\n",
      "Epoch 74/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3798 - acc: 0.2202 - val_loss: 0.4911 - val_acc: 0.2010\n",
      "Epoch 75/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3795 - acc: 0.2203 - val_loss: 0.4931 - val_acc: 0.2002\n",
      "Epoch 76/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3790 - acc: 0.2204 - val_loss: 0.4926 - val_acc: 0.2007\n",
      "Epoch 77/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3788 - acc: 0.2206 - val_loss: 0.4928 - val_acc: 0.2006\n",
      "Epoch 78/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3785 - acc: 0.2206 - val_loss: 0.4925 - val_acc: 0.2007\n",
      "Epoch 79/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3781 - acc: 0.2207 - val_loss: 0.4938 - val_acc: 0.2004\n",
      "Epoch 80/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3778 - acc: 0.2208 - val_loss: 0.4946 - val_acc: 0.1998\n",
      "Epoch 81/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3775 - acc: 0.2209 - val_loss: 0.4944 - val_acc: 0.2003\n",
      "Epoch 82/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3773 - acc: 0.2209 - val_loss: 0.4946 - val_acc: 0.2003\n",
      "Epoch 83/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3768 - acc: 0.2210 - val_loss: 0.4954 - val_acc: 0.2002\n",
      "Epoch 84/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3766 - acc: 0.2211 - val_loss: 0.4961 - val_acc: 0.2001\n",
      "Epoch 85/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3764 - acc: 0.2213 - val_loss: 0.4961 - val_acc: 0.2001\n",
      "Epoch 86/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3760 - acc: 0.2213 - val_loss: 0.4963 - val_acc: 0.2004\n",
      "Epoch 87/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3756 - acc: 0.2214 - val_loss: 0.4974 - val_acc: 0.1997\n",
      "Epoch 88/100\n",
      "57915/57915 [==============================] - 143s 2ms/step - loss: 0.3755 - acc: 0.2215 - val_loss: 0.4987 - val_acc: 0.1994\n",
      "Epoch 89/100\n",
      "57915/57915 [==============================] - 142s 2ms/step - loss: 0.3752 - acc: 0.2216 - val_loss: 0.4973 - val_acc: 0.1994\n",
      "Epoch 90/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3749 - acc: 0.2216 - val_loss: 0.4990 - val_acc: 0.1990\n",
      "Epoch 91/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3747 - acc: 0.2217 - val_loss: 0.4983 - val_acc: 0.1993\n",
      "Epoch 92/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3743 - acc: 0.2219 - val_loss: 0.4986 - val_acc: 0.1997\n",
      "Epoch 93/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3742 - acc: 0.2219 - val_loss: 0.4987 - val_acc: 0.1999\n",
      "Epoch 94/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3739 - acc: 0.2220 - val_loss: 0.4991 - val_acc: 0.1995\n",
      "Epoch 95/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3737 - acc: 0.2220 - val_loss: 0.4994 - val_acc: 0.1996\n",
      "Epoch 96/100\n",
      "57915/57915 [==============================] - 139s 2ms/step - loss: 0.3734 - acc: 0.2221 - val_loss: 0.5001 - val_acc: 0.1990\n",
      "Epoch 97/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3732 - acc: 0.2223 - val_loss: 0.5002 - val_acc: 0.1995\n",
      "Epoch 98/100\n",
      "57915/57915 [==============================] - 140s 2ms/step - loss: 0.3729 - acc: 0.2223 - val_loss: 0.5003 - val_acc: 0.1994\n",
      "Epoch 99/100\n",
      "57915/57915 [==============================] - 144s 2ms/step - loss: 0.3727 - acc: 0.2224 - val_loss: 0.5014 - val_acc: 0.1989\n",
      "Epoch 100/100\n",
      "57915/57915 [==============================] - 145s 3ms/step - loss: 0.3725 - acc: 0.2224 - val_loss: 0.5001 - val_acc: 0.1992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25153fb9e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model  # noqa\n",
    "from keras.layers import Input, LSTM, Dense  # noqa\n",
    "\n",
    "batch_size = 64    # <1>\n",
    "epochs = 100       # <2>\n",
    "num_neurons = 256  # <3>\n",
    "\n",
    "encoder_inputs = Input(shape=(None, len(input_vocab)))\n",
    "encoder = LSTM(num_neurons, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, len(output_vocab)))\n",
    "decoder_lstm = LSTM(num_neurons, return_sequences=True,\n",
    "                    return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(output_vocab), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([encoder_input_onehot, decoder_input_onehot],\n",
    "          decoder_target_onehot, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpia.loaders import get_data, DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/nlpiaenv/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(DATA_PATH, 'ch10_train_movie_dialog_keras')\n",
    "model.save(model_path + '_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_path + '_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_path = os.path.join(DATA_PATH, 'ch10_train_movie_dialog_keras')\n",
    "model = load_model(model_path + '_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_path + '_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/nlpiaenv/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "thought_input = [\n",
    "    Input(shape=(num_neurons,)), Input(shape=(num_neurons,))]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=thought_input)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    inputs=[decoder_inputs] + thought_input,\n",
    "    output=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    thought = encoder_model.predict(input_seq)  # <1>\n",
    "\n",
    "    target_seq = np.zeros((1, 1, len(output_vocab)))  # <2>\n",
    "    target_seq[0, 0, output_vocab.index(stop_token)\n",
    "        ] = 1.  # <3>\n",
    "    stop_condition = False\n",
    "    generated_sequence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + thought) # <4>\n",
    "\n",
    "        generated_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "        generated_char = output_vocab[generated_token_idx]\n",
    "        generated_sequence += generated_char\n",
    "        if (generated_char == stop_token or\n",
    "                len(generated_sequence) > max_decoder_seq_len\n",
    "                ):  # <5>\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, len(output_vocab)))  # <6>\n",
    "        target_seq[0, 0, generated_token_idx] = 1.\n",
    "        thought = [h, c]  # <7>\n",
    "\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(input_text):\n",
    "    input_text = input_text.lower()\n",
    "    input_text = ''.join(c if c in input_vocab else ' ' for c in input_text)\n",
    "    input_seq = np.zeros((1, max_encoder_seq_len, len(input_vocab)), dtype='float32')\n",
    "    for t, c in enumerate(input_text):\n",
    "        input_seq[0, t, input_vocab.index(c)] = 1.\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('Human: {}'.format(input_text))\n",
    "    print('Bot:', decoded_sentence)\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hi rosa, how are you?\n",
      "Bot: yes?\n",
      "\n",
      "Human: hi jim, how are you?\n",
      "Bot: hi.\n",
      "\n",
      "Human: hi barak, how are you?\n",
      "Bot: ...i was all right.\n",
      "\n",
      "Human: hi amy, how are you?\n",
      "Bot: hello, darling.\n",
      "\n",
      "Human: hi paris, how are you?\n",
      "Bot: everything i am.\n",
      "\n",
      "Human: hi joe, how are you?\n",
      "Bot: hi.\n",
      "\n",
      "Human: hi jane, how are you?\n",
      "Bot: hi.\n",
      "\n",
      "Human: hey jane, how are you?\n",
      "Bot: what are you going to do with this ship?\n",
      "\n",
      "Human: hey jon, how are you?\n",
      "Bot: hey, i was a little bit to be a man with you.\n",
      "\n",
      "Human: hey john, how are you?\n",
      "Bot: hey, what are you going to do to say?\n",
      "\n",
      "Human: hey joe, how are you?\n",
      "Bot: hey, i was a little bit to be a man with you.\n",
      "\n",
      "Human: hey jim, how are you?\n",
      "Bot: hey, man! you're gonna die what i say.\n",
      "\n",
      "Human: hey ashley, how are you?\n",
      "Bot: hey, what are you going to do to say?\n",
      "\n",
      "Human: hey my love, how are you?\n",
      "Bot: yes.\n",
      "\n",
      "Human: hey arzu, how are you?\n",
      "Bot: a start.\n",
      "\n",
      "Human: i'm talking about us.\n",
      "Bot: it's a long time ago. i think i should think i was a little late.\n",
      "\n",
      "Human: what are you trying to say?\n",
      "Bot: i was a company on the street.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i was a company on the street.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respond('Hi Rosa, how are you?')\n",
    "respond('Hi Jim, how are you?')\n",
    "respond('Hi Barak, how are you?')\n",
    "respond('Hi Amy, how are you?')\n",
    "respond('Hi Paris, how are you?')\n",
    "respond('Hi Joe, how are you?')\n",
    "respond('Hi Jane, how are you?')\n",
    "respond('Hey Jane, how are you?')\n",
    "respond('Hey Jon, how are you?')\n",
    "respond('Hey John, how are you?')\n",
    "respond('Hey Joe, how are you?')\n",
    "respond('Hey Jim, how are you?')\n",
    "respond('Hey Ashley, how are you?')\n",
    "respond('Hey my love, how are you?')\n",
    "respond('Hey Arzu, how are you?')\n",
    "respond(\"I'm talking about us.\")\n",
    "respond(\"What are you trying to say?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpiaenv",
   "language": "python",
   "name": "nlpiaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
