{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import textract\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = KeyedVectors.load_word2vec_format('~/data/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n",
    "word_vectors = KeyedVectors.load_word2vec_format('~/data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cook', 0.6973531246185303),\n",
       " ('oven_roasting', 0.6754531860351562),\n",
       " ('Slow_cooker', 0.6742031574249268),\n",
       " ('sweet_potatoes', 0.6600280404090881),\n",
       " ('stir_fry_vegetables', 0.6548759341239929)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['cooking', 'potatoes'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.doesnt_match(\"potatoes milk cake computer\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('europe', 0.7222039699554443)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['germany', 'france'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7070532"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('princess', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01446533, -0.12792969, -0.11572266, -0.22167969, -0.07373047,\n",
       "       -0.05981445, -0.10009766, -0.06884766,  0.14941406,  0.10107422],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['phone'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/mavis-batey-sentences.txt\", \"r\")\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a strange little outfit in the cottage.\\nOrganisation is not a word you would associate with Dilly Knox.\\nWhen I arrived, he said: \"Oh, hello, we\\'re breaking machines, have you got a pencil?\"\\nThat was it.\\nI was never really told what to do.\\nI think, looking back on it, that was a great precedent in my life, because he taught me to think that you could do things yourself without always checking up to see what the book said.\\nThat was the way the cottage worked.\\nWe were looking at new traffic all the time or where the wheels or the wiring had been changed, or at other new techniques.\\nSo you had to work it all out yourself from scratch.\\nWhy they had to say that (\"Today\\'s the day minus three.\") I can\\'t imagine.\\nIt seems rather daft, but they did.\\nSo we worked for three days.\\nIt was all the nail-biting stuff of keeping up all night working.\\nOne kept thinking: \"Well, would one be better at it if one had a little sleep or shall we just go on?\"--and it did take nearly all of three days.\\nThen a very, very large message came in.\\nHow many cruisers there were, and how many submarines were to be there and where they were to be at such and such a time, absolutely incredible that they should spell it all out.\\nHe pretended he was just going to have the weekend off and made sure the Japanese spy would pass it all back.\\nThen, under cover of the night, they went out and confronted the Italians.\\nIt was very exciting stuff.\\nThere was a great deal of jubilation in the cottage and then Cunningham himself came to visit us to congratulate us in person.\\nThe cottage wall had just been whitewashed.\\nNow this just shows how silly and young and giggly we were.\\nWe thought it would be jolly funny if we could talk to Admiral Cunningham and get him to lean against the wet whitewash and go away with a white stern.\\nHe would ask new arrivals which way the hands of a clock went round.\\nNot if you\\'re inside the clock.\\nI picked up this message and thought: \"There is not a single L in this message.\"\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation = ['(',')',';',':','[',']',',', '-', '/','.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vector = []\n",
    "for sentence in sentences:\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    normalized_tokens = [x.lower() for x in tokens if x not in punctuation and x not in stop_words]\n",
    "    sentence_vector.append(normalized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it', 'strange', 'little', 'outfit', 'cottage'],\n",
       " ['organisation', 'word', 'would', 'associate', 'dilly', 'knox'],\n",
       " ['when',\n",
       "  'i',\n",
       "  'arrived',\n",
       "  'said',\n",
       "  '``',\n",
       "  'oh',\n",
       "  'hello',\n",
       "  \"'re\",\n",
       "  'breaking',\n",
       "  'machines',\n",
       "  'got',\n",
       "  'pencil',\n",
       "  '?',\n",
       "  \"''\"],\n",
       " ['that'],\n",
       " ['i', 'never', 'really', 'told'],\n",
       " ['i',\n",
       "  'think',\n",
       "  'looking',\n",
       "  'back',\n",
       "  'great',\n",
       "  'precedent',\n",
       "  'life',\n",
       "  'taught',\n",
       "  'think',\n",
       "  'could',\n",
       "  'things',\n",
       "  'without',\n",
       "  'always',\n",
       "  'checking',\n",
       "  'see',\n",
       "  'book',\n",
       "  'said'],\n",
       " ['that', 'way', 'cottage', 'worked'],\n",
       " ['we',\n",
       "  'looking',\n",
       "  'new',\n",
       "  'traffic',\n",
       "  'time',\n",
       "  'wheels',\n",
       "  'wiring',\n",
       "  'changed',\n",
       "  'new',\n",
       "  'techniques'],\n",
       " ['so', 'work', 'scratch'],\n",
       " ['why',\n",
       "  'say',\n",
       "  '``',\n",
       "  'today',\n",
       "  \"'s\",\n",
       "  'day',\n",
       "  'minus',\n",
       "  'three.',\n",
       "  \"''\",\n",
       "  'i',\n",
       "  'ca',\n",
       "  \"n't\",\n",
       "  'imagine'],\n",
       " ['it', 'seems', 'rather', 'daft'],\n",
       " ['so', 'worked', 'three', 'days'],\n",
       " ['it', 'nail-biting', 'stuff', 'keeping', 'night', 'working'],\n",
       " ['one',\n",
       "  'kept',\n",
       "  'thinking',\n",
       "  '``',\n",
       "  'well',\n",
       "  'would',\n",
       "  'one',\n",
       "  'better',\n",
       "  'one',\n",
       "  'little',\n",
       "  'sleep',\n",
       "  'shall',\n",
       "  'go',\n",
       "  '?',\n",
       "  \"''\",\n",
       "  '--',\n",
       "  'take',\n",
       "  'nearly',\n",
       "  'three',\n",
       "  'days'],\n",
       " ['then', 'large', 'message', 'came'],\n",
       " ['how',\n",
       "  'many',\n",
       "  'cruisers',\n",
       "  'many',\n",
       "  'submarines',\n",
       "  'time',\n",
       "  'absolutely',\n",
       "  'incredible',\n",
       "  'spell'],\n",
       " ['he',\n",
       "  'pretended',\n",
       "  'going',\n",
       "  'weekend',\n",
       "  'made',\n",
       "  'sure',\n",
       "  'japanese',\n",
       "  'spy',\n",
       "  'would',\n",
       "  'pass',\n",
       "  'back'],\n",
       " ['then', 'cover', 'night', 'went', 'confronted', 'italians'],\n",
       " ['it', 'exciting', 'stuff'],\n",
       " ['there',\n",
       "  'great',\n",
       "  'deal',\n",
       "  'jubilation',\n",
       "  'cottage',\n",
       "  'cunningham',\n",
       "  'came',\n",
       "  'visit',\n",
       "  'us',\n",
       "  'congratulate',\n",
       "  'us',\n",
       "  'person'],\n",
       " ['the', 'cottage', 'wall', 'whitewashed'],\n",
       " ['now', 'shows', 'silly', 'young', 'giggly'],\n",
       " ['we',\n",
       "  'thought',\n",
       "  'would',\n",
       "  'jolly',\n",
       "  'funny',\n",
       "  'could',\n",
       "  'talk',\n",
       "  'admiral',\n",
       "  'cunningham',\n",
       "  'get',\n",
       "  'lean',\n",
       "  'wet',\n",
       "  'whitewash',\n",
       "  'go',\n",
       "  'away',\n",
       "  'white',\n",
       "  'stern'],\n",
       " ['he',\n",
       "  'would',\n",
       "  'ask',\n",
       "  'new',\n",
       "  'arrivals',\n",
       "  'way',\n",
       "  'hands',\n",
       "  'clock',\n",
       "  'went',\n",
       "  'round'],\n",
       " ['not', \"'re\", 'inside', 'clock'],\n",
       " ['i',\n",
       "  'picked',\n",
       "  'message',\n",
       "  'thought',\n",
       "  '``',\n",
       "  'there',\n",
       "  'single',\n",
       "  'l',\n",
       "  'message',\n",
       "  \"''\"],\n",
       " []]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "num_features = 300\n",
    "min_word_count = 3\n",
    "num_workers = 2\n",
    "window_size = 6\n",
    "subsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-07 18:03:44,123 WARNING:gensim.models.base_any2vec:1386:      _log_train_end under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentence_vector, workers=num_workers, size=num_features, min_count=min_word_count, window=window_size, sample=subsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discard the unneeded output weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mavis-batey-sentences'\n",
    "model.save('mavis-batey-sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=9, size=300, alpha=0.025)\n",
      "['it', 'cottage', 'would', 'i', '``', \"''\", 'new', 'one', 'message']\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [ \"the quick brown fox jumps over the lazy dogs\",\n",
    "\"Then a cop quizzed Mick Jagger's ex-wives briefly.\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [s.split() for s in test_sentences] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'],\n",
       " ['Then', 'a', 'cop', 'quizzed', 'Mick', \"Jagger's\", 'ex-wives', 'briefly.']]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-07 18:49:28,145 WARNING:gensim.models.base_any2vec:1386:      _log_train_end under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "Word2Vec(vocab=150, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentence_vector, min_count=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'strange', 'little', 'outfit', 'cottage', 'organisation', 'word', 'would', 'associate', 'dilly', 'knox', 'when', 'i', 'arrived', 'said', '``', 'oh', 'hello', \"'re\", 'breaking', 'machines', 'got', 'pencil', '?', \"''\", 'that', 'never', 'really', 'told', 'think', 'looking', 'back', 'great', 'precedent', 'life', 'taught', 'could', 'things', 'without', 'always', 'checking', 'see', 'book', 'way', 'worked', 'we', 'new', 'traffic', 'time', 'wheels', 'wiring', 'changed', 'techniques', 'so', 'work', 'scratch', 'why', 'say', 'today', \"'s\", 'day', 'minus', 'three.', 'ca', \"n't\", 'imagine', 'seems', 'rather', 'daft', 'three', 'days', 'nail-biting', 'stuff', 'keeping', 'night', 'working', 'one', 'kept', 'thinking', 'well', 'better', 'sleep', 'shall', 'go', '--', 'take', 'nearly', 'then', 'large', 'message', 'came', 'how', 'many', 'cruisers', 'submarines', 'absolutely', 'incredible', 'spell', 'he', 'pretended', 'going', 'weekend', 'made', 'sure', 'japanese', 'spy', 'pass', 'cover', 'went', 'confronted', 'italians', 'exciting', 'there', 'deal', 'jubilation', 'cunningham', 'visit', 'us', 'congratulate', 'person', 'the', 'wall', 'whitewashed', 'now', 'shows', 'silly', 'young', 'giggly', 'thought', 'jolly', 'funny', 'talk', 'admiral', 'get', 'lean', 'wet', 'whitewash', 'away', 'white', 'stern', 'ask', 'arrivals', 'hands', 'clock', 'round', 'not', 'inside', 'picked', 'single', 'l']\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "vocab = pd.Series(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vocab.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <td>Vocab(count:3000000, index:0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>Vocab(count:2999999, index:1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>Vocab(count:2999998, index:2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>Vocab(count:2999997, index:3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>Vocab(count:2999996, index:4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0\n",
       "</s>  Vocab(count:3000000, index:0)\n",
       "in    Vocab(count:2999999, index:1)\n",
       "for   Vocab(count:2999998, index:2)\n",
       "that  Vocab(count:2999997, index:3)\n",
       "is    Vocab(count:2999996, index:4)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Starwood_Hotels_HOT       Vocab(count:2000000, index:1000000)\n",
       "Tammy_Kilborn             Vocab(count:1999999, index:1000001)\n",
       "aortic_aneurism           Vocab(count:1999998, index:1000002)\n",
       "Spragins_Hall             Vocab(count:1999997, index:1000003)\n",
       "Ed_Iacobucci              Vocab(count:1999996, index:1000004)\n",
       "Seilheimer                Vocab(count:1999995, index:1000005)\n",
       "Frank_Della_Femina        Vocab(count:1999994, index:1000006)\n",
       "egoli                     Vocab(count:1999993, index:1000007)\n",
       "Brivik                    Vocab(count:1999992, index:1000008)\n",
       "actress_Hema_Malini       Vocab(count:1999991, index:1000009)\n",
       "singer_Angelique_Kidjo    Vocab(count:1999990, index:1000010)\n",
       "Authority_PDMA            Vocab(count:1999989, index:1000011)\n",
       "Shapp                     Vocab(count:1999988, index:1000012)\n",
       "Joris_den_Blanken         Vocab(count:1999987, index:1000013)\n",
       "Pascal_Berenguer          Vocab(count:1999986, index:1000014)\n",
       "Dick_LaHaie               Vocab(count:1999985, index:1000015)\n",
       "M._FELIPE_Public          Vocab(count:1999984, index:1000016)\n",
       "Kanemaru                  Vocab(count:1999983, index:1000017)\n",
       "Rooftop_Comedy            Vocab(count:1999982, index:1000018)\n",
       "germinating_seeds         Vocab(count:1999981, index:1000019)\n",
       "dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.iloc[1000000:1000020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15625   ,  0.18652344,  0.33203125,  0.55859375,  0.03637695,\n",
       "       -0.09375   , -0.05029297,  0.16796875, -0.0625    ,  0.09912109],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['Illini'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.36538"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance = np.linalg.norm(word_vectors['Illinois'] - word_vectors['Illini'])\n",
    "euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity = np.dot(wv['Illinois'], wv['Illini']) / (np.linalg.norm(wv['Illinois']) * np.linalg.norm(wv['Illini']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5501352"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44986480474472046"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpia.data.loaders import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2051/2051 [00:01<00:00, 1118.37it/s]\n"
     ]
    }
   ],
   "source": [
    "cities = get_data('cities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>geonameid</th>\n",
       "      <th>3039154</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>El Tarter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asciiname</th>\n",
       "      <td>El Tarter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alternatenames</th>\n",
       "      <td>Ehl Tarter,Эл Тартер</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>42.5795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>1.65362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_class</th>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_code</th>\n",
       "      <td>PPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country_code</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admin1_code</th>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admin2_code</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admin3_code</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admin4_code</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elevation</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem</th>\n",
       "      <td>1721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timezone</th>\n",
       "      <td>Europe/Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modification_date</th>\n",
       "      <td>2012-11-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "geonameid                       3039154\n",
       "name                          El Tarter\n",
       "asciiname                     El Tarter\n",
       "alternatenames     Ehl Tarter,Эл Тартер\n",
       "latitude                        42.5795\n",
       "longitude                       1.65362\n",
       "feature_class                         P\n",
       "feature_code                        PPL\n",
       "country_code                         AD\n",
       "cc2                                 NaN\n",
       "admin1_code                          02\n",
       "admin2_code                         NaN\n",
       "admin3_code                         NaN\n",
       "admin4_code                         NaN\n",
       "population                         1052\n",
       "elevation                           NaN\n",
       "dem                                1721\n",
       "timezone                 Europe/Andorra\n",
       "modification_date            2012-11-03"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = cities[(cities.country_code == 'US') & (cities.admin1_code.notnull())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.read_csv('http://www.fonz.net/blog/wp-content/uploads/2008/04/states.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = dict(zip(states.Abbreviation, states.State))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>st</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geonameid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4046255</th>\n",
       "      <td>Bay Minette</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046274</th>\n",
       "      <td>Edna</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046319</th>\n",
       "      <td>Bayou La Batre</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046332</th>\n",
       "      <td>Henderson</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046430</th>\n",
       "      <td>Natalia</td>\n",
       "      <td>TX</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     city  st    state\n",
       "geonameid                             \n",
       "4046255       Bay Minette  AL  Alabama\n",
       "4046274              Edna  TX    Texas\n",
       "4046319    Bayou La Batre  AL  Alabama\n",
       "4046332         Henderson  TX    Texas\n",
       "4046430           Natalia  TX    Texas"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us['city'] = us.name.copy()\n",
    "us['st'] = us.admin1_code.copy()\n",
    "us['state'] = us.st.map(states)\n",
    "us[us.columns[-3:]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Edna', 'Henderson', 'Natalia', 'Yorktown', 'Brighton', 'Berry',\n",
       "       'Trinity', 'Villas', 'Bessemer', 'Aurora'], dtype='<U15')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = pd.np.concatenate([us.city, us.st, us.state])\n",
    "vocab = np.array([word for word in vocab if word in wv.wv])\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_plus_state = []\n",
    "for c, state, st in zip(us.city, us.state, us.st):\n",
    "    if c not in vocab:\n",
    "        continue\n",
    "    row = []\n",
    "    if state in vocab:\n",
    "        row.extend(wv[c] + wv[state])\n",
    "    else:\n",
    "        row.extend(wv[c] + wv[st])\n",
    "    city_plus_state.append(row)\n",
    "us_300D = pd.DataFrame(city_plus_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.346680</td>\n",
       "      <td>-0.254517</td>\n",
       "      <td>-0.158203</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>0.107910</td>\n",
       "      <td>-0.284180</td>\n",
       "      <td>-0.552734</td>\n",
       "      <td>-0.059082</td>\n",
       "      <td>-0.484375</td>\n",
       "      <td>0.087402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.082275</td>\n",
       "      <td>-0.322510</td>\n",
       "      <td>-0.006836</td>\n",
       "      <td>0.252930</td>\n",
       "      <td>-0.262207</td>\n",
       "      <td>-0.132812</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>0.079834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145996</td>\n",
       "      <td>0.076050</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>0.411133</td>\n",
       "      <td>0.231934</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.387939</td>\n",
       "      <td>-0.011963</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>-0.051270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111572</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>-0.019043</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.301758</td>\n",
       "      <td>-0.343750</td>\n",
       "      <td>-0.033691</td>\n",
       "      <td>-0.061523</td>\n",
       "      <td>-0.088379</td>\n",
       "      <td>0.088135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007324</td>\n",
       "      <td>-0.397095</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>-0.007080</td>\n",
       "      <td>-0.129395</td>\n",
       "      <td>-0.299316</td>\n",
       "      <td>-0.253174</td>\n",
       "      <td>-0.471680</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161621</td>\n",
       "      <td>-0.131836</td>\n",
       "      <td>-0.075439</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>0.459961</td>\n",
       "      <td>-0.212646</td>\n",
       "      <td>0.127686</td>\n",
       "      <td>0.309570</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.206055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.248535</td>\n",
       "      <td>-0.042969</td>\n",
       "      <td>-0.239746</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>-0.211426</td>\n",
       "      <td>-0.305908</td>\n",
       "      <td>-0.826172</td>\n",
       "      <td>0.292053</td>\n",
       "      <td>-0.260254</td>\n",
       "      <td>0.311035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125427</td>\n",
       "      <td>-0.017090</td>\n",
       "      <td>-0.448242</td>\n",
       "      <td>0.145752</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.297852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210693</td>\n",
       "      <td>-0.336426</td>\n",
       "      <td>-0.231445</td>\n",
       "      <td>0.532715</td>\n",
       "      <td>-0.035583</td>\n",
       "      <td>0.104736</td>\n",
       "      <td>-0.518555</td>\n",
       "      <td>0.189392</td>\n",
       "      <td>-0.192383</td>\n",
       "      <td>0.263184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160034</td>\n",
       "      <td>0.213440</td>\n",
       "      <td>-0.556641</td>\n",
       "      <td>-0.168213</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>-0.090576</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>-0.204102</td>\n",
       "      <td>-0.116455</td>\n",
       "      <td>-0.086182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.346680 -0.254517 -0.158203  0.163086  0.107910 -0.284180 -0.552734   \n",
       "1  0.145996  0.076050 -0.058594  0.411133  0.231934 -0.116699 -0.387939   \n",
       "2  0.007324 -0.397095  0.134277  0.263672 -0.007080 -0.129395 -0.299316   \n",
       "3  0.248535 -0.042969 -0.239746  0.006104 -0.211426 -0.305908 -0.826172   \n",
       "4  0.210693 -0.336426 -0.231445  0.532715 -0.035583  0.104736 -0.518555   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0 -0.059082 -0.484375  0.087402  ...  0.028320  0.082275 -0.322510 -0.006836   \n",
       "1 -0.011963 -0.200195 -0.051270  ...  0.111572  0.146484 -0.019043  0.274414   \n",
       "2 -0.253174 -0.471680  0.120117  ... -0.161621 -0.131836 -0.075439 -0.019531   \n",
       "3  0.292053 -0.260254  0.311035  ... -0.125427 -0.017090 -0.448242  0.145752   \n",
       "4  0.189392 -0.192383  0.263184  ... -0.160034  0.213440 -0.556641 -0.168213   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.252930 -0.262207 -0.132812  0.226562  0.146484  0.079834  \n",
       "1  0.301758 -0.343750 -0.033691 -0.061523 -0.088379  0.088135  \n",
       "2  0.459961 -0.212646  0.127686  0.309570  0.024414  0.206055  \n",
       "3  0.350586 -0.593750  0.257812  0.110352  0.010742  0.297852  \n",
       "4  0.027344 -0.090576  0.016602 -0.204102 -0.116455 -0.086182  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_300D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2064/2064 [00:01<00:00, 1122.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "us_300D = get_data('cities_us_wordvectors')\n",
    "us_2D = pca.fit_transform(us_300D.iloc[:, :300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "from nlpia.plots import offline_plotly_scatter_bubble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data('cities_us_wordvectors_pca2_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>name</th>\n",
       "      <th>population</th>\n",
       "      <th>timezone</th>\n",
       "      <th>state</th>\n",
       "      <th>state_abbreviation</th>\n",
       "      <th>elevation</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation_m</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Los Angeles, CA</th>\n",
       "      <td>4.930810</td>\n",
       "      <td>-0.936239</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>3971883</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>CA</td>\n",
       "      <td>89.0</td>\n",
       "      <td>34.05223</td>\n",
       "      <td>-118.24368</td>\n",
       "      <td>96</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicago, IL</th>\n",
       "      <td>-2.021456</td>\n",
       "      <td>-2.615556</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>2720546</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>IL</td>\n",
       "      <td>179.0</td>\n",
       "      <td>41.85003</td>\n",
       "      <td>-87.65005</td>\n",
       "      <td>180</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Houston, TX</th>\n",
       "      <td>1.339572</td>\n",
       "      <td>2.576390</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>2296224</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>Texas</td>\n",
       "      <td>TX</td>\n",
       "      <td>12.0</td>\n",
       "      <td>29.76328</td>\n",
       "      <td>-95.36327</td>\n",
       "      <td>30</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philadelphia, PA</th>\n",
       "      <td>-1.217258</td>\n",
       "      <td>-1.380793</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>1567442</td>\n",
       "      <td>America/New_York</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>PA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>39.95233</td>\n",
       "      <td>-75.16379</td>\n",
       "      <td>40</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix, AZ</th>\n",
       "      <td>2.046930</td>\n",
       "      <td>-0.480845</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>1563025</td>\n",
       "      <td>America/Phoenix</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>AZ</td>\n",
       "      <td>331.0</td>\n",
       "      <td>33.44838</td>\n",
       "      <td>-112.07404</td>\n",
       "      <td>366</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         x         y              name  population  \\\n",
       "Los Angeles, CA   4.930810 -0.936239   Los Angeles, CA     3971883   \n",
       "Chicago, IL      -2.021456 -2.615556       Chicago, IL     2720546   \n",
       "Houston, TX       1.339572  2.576390       Houston, TX     2296224   \n",
       "Philadelphia, PA -1.217258 -1.380793  Philadelphia, PA     1567442   \n",
       "Phoenix, AZ       2.046930 -0.480845       Phoenix, AZ     1563025   \n",
       "\n",
       "                             timezone         state state_abbreviation  \\\n",
       "Los Angeles, CA   America/Los_Angeles    California                 CA   \n",
       "Chicago, IL           America/Chicago      Illinois                 IL   \n",
       "Houston, TX           America/Chicago         Texas                 TX   \n",
       "Philadelphia, PA     America/New_York  Pennsylvania                 PA   \n",
       "Phoenix, AZ           America/Phoenix       Arizona                 AZ   \n",
       "\n",
       "                  elevation  latitude  longitude  elevation_m country_code  \n",
       "Los Angeles, CA        89.0  34.05223 -118.24368           96           US  \n",
       "Chicago, IL           179.0  41.85003  -87.65005          180           US  \n",
       "Houston, TX            12.0  29.76328  -95.36327           30           US  \n",
       "Philadelphia, PA       12.0  39.95233  -75.16379           40           US  \n",
       "Phoenix, AZ           331.0  33.44838 -112.07404          366           US  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = offline_plotly_scatter_bubble(\n",
    "    df.sort_values('population', ascending=False)[:350].copy().sort_values('population'),\n",
    "    filename='plotly_scatter_bubble.html',\n",
    "    x='x', y='y',\n",
    "    size_col='population', text_col='name', category_col='timezone',\n",
    "    xscale=None, yscale=None, # 'log' or None\n",
    "    layout={}, marker={'sizeref': 3000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-12 13:22:50,659 WARNING:gensim.models.base_any2vec:1386:      _log_train_end under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "corpus = ['This is the first document ...', 'another document ...']\n",
    "training_corpus = []\n",
    "for i, text in enumerate(corpus):\n",
    "    tagged_doc = TaggedDocument(simple_preprocess(text), [i])\n",
    "    training_corpus.append(tagged_doc)\n",
    "model = Doc2Vec(size=100, min_count=2, workers=num_cores, iter=10)\n",
    "model.build_vocab(training_corpus)\n",
    "model.train(training_corpus, total_examples=model.corpus_count,epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabulary.min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.3305167e-04, -4.1156127e-03,  4.3367036e-03,  2.9815983e-03,\n",
       "        4.8823748e-03, -4.4917064e-03,  9.4660651e-04,  1.4293566e-03,\n",
       "        2.2575750e-03,  1.0674573e-03,  3.0639439e-03, -3.2414796e-03,\n",
       "       -2.1288441e-03,  1.5201317e-03,  1.7416221e-03,  8.8257121e-04,\n",
       "       -2.6882859e-04,  3.0692115e-03, -6.5166439e-04,  1.9648245e-03,\n",
       "        3.3264223e-03, -4.1387873e-03, -7.4989686e-04, -1.5501132e-03,\n",
       "        3.8710143e-05, -7.8900350e-04, -1.2922015e-03,  3.3742979e-03,\n",
       "       -1.6506874e-03, -1.6289792e-04, -9.3871640e-05, -4.6166452e-03,\n",
       "       -3.3299488e-03, -1.5817411e-03, -2.8174839e-03,  4.0226053e-03,\n",
       "       -4.9695540e-03, -2.3478176e-03,  1.1161816e-03,  4.6355836e-03,\n",
       "       -1.6166740e-04,  2.9576819e-03,  2.0952635e-03, -3.3852556e-03,\n",
       "       -1.9980472e-04,  3.4908408e-05, -1.8179971e-03,  2.7284708e-03,\n",
       "       -3.6137220e-03,  3.2053569e-03, -3.1862042e-03,  8.2726497e-04,\n",
       "        4.4547595e-04,  2.2719365e-03, -2.6379526e-03,  1.1983778e-03,\n",
       "       -1.2855318e-03, -3.0439966e-03,  4.9260291e-03,  2.0750479e-03,\n",
       "       -8.4590958e-04, -4.3055252e-03, -2.0274641e-03, -4.2229868e-03,\n",
       "        4.5601963e-03, -2.8031962e-03,  6.4563385e-04, -2.5007604e-03,\n",
       "        1.2794124e-03,  4.5897882e-03, -1.8479490e-03, -1.8108416e-03,\n",
       "        4.0776329e-03, -2.8522939e-03,  4.5695892e-03, -2.1486813e-03,\n",
       "       -4.9803182e-03,  4.3706964e-03, -4.3513381e-04,  1.2757460e-03,\n",
       "       -3.0057437e-03,  1.2729161e-03,  8.0382446e-04, -1.2258949e-03,\n",
       "        2.1530432e-03,  1.0906854e-03, -3.5657240e-03, -4.3336302e-03,\n",
       "        3.6191987e-03, -1.9111605e-04,  9.0628472e-04,  4.5072037e-04,\n",
       "       -3.9413236e-03,  1.2270103e-03, -3.1361568e-03,  7.7159420e-06,\n",
       "        2.5030866e-03,  1.2054760e-03,  4.2230161e-03,  2.4315284e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(simple_preprocess('This is a completely unseen document'), steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpiaenv",
   "language": "python",
   "name": "nlpiaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
